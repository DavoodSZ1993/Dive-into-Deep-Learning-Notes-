{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPTlEybl1pIT0xAVwA7W4B4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DavoodSZ1993/Dive-into-Deep-Learning-Notes-/blob/main/10_7_Encoder_Decoder_Machine_Translation_notes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JGly9mBzOLvA",
        "outputId": "9283aa02-d65e-4082-faec-c1d9a8b33270"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.0/93.0 KB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.0/121.0 KB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.6/83.6 KB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m40.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install d2l==1.0.0-alpha1.post0 --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10.7 Encoder-Decoder Seq2Seq for Machine Translation"
      ],
      "metadata": {
        "id": "ELgg6IAMOSJR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10.7.2 Encoder"
      ],
      "metadata": {
        "id": "q2PE_PYhVnZs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "input_size, hidden_size, num_layers = 10, 20, 2\n",
        "rnn = nn.GRU(input_size, hidden_size, num_layers)\n",
        "\n",
        "rnn._flat_weights_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eb4UJzfAVsqm",
        "outputId": "600daffd-01da-4f36-87c1-0e41efff4a7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['weight_ih_l0',\n",
              " 'weight_hh_l0',\n",
              " 'bias_ih_l0',\n",
              " 'bias_hh_l0',\n",
              " 'weight_ih_l1',\n",
              " 'weight_hh_l1',\n",
              " 'bias_ih_l1',\n",
              " 'bias_hh_l1']"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rnn._parameters"
      ],
      "metadata": {
        "id": "aOOq8k38WtQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Embedding\n",
        "* Embedding layer is equivalent to a linear layer without the bias term. However, embedding does a lookup instead of a matrix-vector multiplication.\n",
        "\n",
        "* An embedding is an efficient alternative way to a single linear layer when one has a large number of input features. This may happen in natural language processing (NLP) when one is working with text data.\n",
        "\n",
        "* Class `torch.nn.Embedding(num_embeddings, embedding_dim)`: A simple lookup table that stores embeddings of a fixed dictionary and size.\n",
        "\n",
        "* This module is often used to score word embeddings and retrieve them using indices. The input to the module is a list of indices, and the output is the corresponding word embeddings.\n",
        "\n",
        "**Parameters:**\n",
        "\n",
        "* *num_embeddings(int)*: Size of the dictionary of embeddings.\n",
        "* *embedding_dim(int)*: The size of each embedding vector.\n",
        "\n",
        "**Shape:**\n",
        "\n",
        "* `input: (*)`: IntTensor or LongTensor of arbitrary shape containing the indices to extract.\n",
        "* `output(*, H)`: Where * is the input shape and H is `embedding_dim`. "
      ],
      "metadata": {
        "id": "8xyNSVT0XxIS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size, embed_size = 10, 8\n",
        "\n",
        "embedding = nn.Embedding(vocab_size, embed_size)"
      ],
      "metadata": {
        "id": "G6gKHt74e3kC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size, num_steps = 4, 9\n",
        "X = torch.zeros((batch_size, num_steps))\n",
        "\n",
        "embs = embedding(X.t().type(torch.int32))\n",
        "embs[0,:,:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOZGrMaMfrrE",
        "outputId": "bc350789-bdd8-4118-9ddd-ae1c052a28e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.1544,  0.7103,  0.2287,  1.2542,  0.0098,  0.2911, -0.1725, -1.0152],\n",
              "        [-0.1544,  0.7103,  0.2287,  1.2542,  0.0098,  0.2911, -0.1725, -1.0152],\n",
              "        [-0.1544,  0.7103,  0.2287,  1.2542,  0.0098,  0.2911, -0.1725, -1.0152],\n",
              "        [-0.1544,  0.7103,  0.2287,  1.2542,  0.0098,  0.2911, -0.1725, -1.0152]],\n",
              "       grad_fn=<SliceBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### In-depth Look at `nn.Embedding()`\n",
        "* The `nn.Embedding()` layer is a simple lookup table that maps an index value to a wieght matrix of a certain dimension.\n",
        "\n",
        "* This simple operation is the foundation of many advanced NLP architectures, allowing for the processing of discrete input symbols in a continuous state.\n",
        "\n",
        "* The *nn.Embedding* layer takes at least two arguments, **the vocabulary size** and **the size of the encoded representation for each word**.\n",
        "\n",
        "* Lets say we have a vocabulary of 1000 words, then the first argument would be 1000. Each word in the vocabulary will be represented by a vector of fixed size. So, the second argument is the size of the learned embedding for each word."
      ],
      "metadata": {
        "id": "Fkbf5BttVlJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size, embed_size = 10, 50\n",
        "\n",
        "embedding = nn.Embedding(vocab_size, embed_size)"
      ],
      "metadata": {
        "id": "Nl_m43L-MnnF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above example, PyTorch is created a lookup table named `embedding` that has 10 rows and 50 columns. Each row represents a single word embedding that is initialized randomly drawn from a uniform distribution. They are initialized using `nn.init.uniform_()` function from the `torch.nn.init` module and weights are initialized with random values between -1 and 1."
      ],
      "metadata": {
        "id": "z5UBX2pWNNr7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding(torch.LongTensor([0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50hYINUUOqAO",
        "outputId": "844af982-0981-49ea-ea07-dc416b695122"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.8511,  0.1879,  0.7696,  0.6570, -1.3819,  1.5425,  1.2204,  1.3877,\n",
              "         -0.1510, -1.0491,  0.0389, -0.0055, -0.2214,  1.0159,  0.8809,  0.5047,\n",
              "         -1.0305, -0.2009,  1.4088, -1.9277, -1.5470, -0.8555,  0.1258,  0.8923,\n",
              "          1.4227,  0.9337, -0.6947, -0.0893,  0.0046,  1.4121,  0.6984, -1.4294,\n",
              "          0.0077,  0.1171,  1.0447, -1.2901,  0.9510,  1.2457,  1.4315,  0.9397,\n",
              "          1.4418, -0.4111,  1.3055, -0.7426, -0.7994, -0.3499,  0.5569, -0.8989,\n",
              "         -0.5761,  1.3471]], grad_fn=<EmbeddingBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above numbers get tuned and optimized during the training process to convey the meaning of a certain word."
      ],
      "metadata": {
        "id": "f_3-svn5SH-5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* `torch.tensor.repeat()`: Repeats a tensor along the specified dimensions."
      ],
      "metadata": {
        "id": "xk3w-1k7Ujtx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "X = torch.tensor([1, 2])\n",
        "\n",
        "X.repeat(2, 2), X.repeat(2, 2).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xy_ccMQiVIqA",
        "outputId": "4e9944b6-a43e-4024-d1d3-9dff25af7c01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[1, 2, 1, 2],\n",
              "         [1, 2, 1, 2]]), torch.Size([2, 4]))"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `torch.optim` package\n",
        "This is a package implementing various optimization algorithms."
      ],
      "metadata": {
        "id": "8J1Gk0nuX_R-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adam Optimizer\n",
        "* The Adam optimizer algorithm is an extension to stochastic gradient descent.\n",
        "* Stochastic gradient descent maintains a single learning rate for all weight updates and does not change during training.\n",
        "* In Adam, a learning rate is maintained for each network weight (parameter) and seperately adapted as learning unfolds.\n",
        "* Adam exploits the advantages of **Adaptive Gradient Algorithm** and **Root Mean Square Propagation** which are two extensions of stochastic gradient descent.\n",
        "* **Adaptive Gradient Algorithm**: maintains a per-parameter learning rate that improves performance on problems with sparse gradients (e.g. natural language processing, and computer vision problems.)\n",
        "* **Root Mean Square Propagation**: also maintains a per=parameter learning rate that are adapted based on the average of recent magnitudes of the gradients for the weight (e.g. how quickly it is changing).\n",
        "* In Adam, instead of adapting the parameter learning rates based on the average first moment (the mean) as in **Root Mean Square Propogation**, the algorithm makes use of the average of the second moments of the gradients (the uncentered variance).\n",
        "* Adam algorithm calculates an exponential moving average of the gradient and the squared gradient, and the parameters `beta1` and `beta2` control the decay rates of the moving averages.\n",
        "* Class `torch.optim.Adam()` Implements Adam algorithm."
      ],
      "metadata": {
        "id": "IOUqt-L0cfwD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* `torch.unsqueeze(input, dim)`: Returns a new tensor with a dimension of size one inserted at the specified position."
      ],
      "metadata": {
        "id": "bxSAFHorlIee"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "X = torch.tensor([1, 2])\n",
        "Y = X.unsqueeze(0)\n",
        "Z = X.unsqueeze(1)\n",
        "\n",
        "X.shape, Y.shape, Z.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bd9onVRZlumH",
        "outputId": "5b9fc92f-fc74-4d6b-a079-943c6653de0d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([2]), torch.Size([1, 2]), torch.Size([2, 1]))"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* `torch.squeeze()`: Returns a tensor with all the dimensions of `input` of size 1 is removed.\n",
        "* For example, if input is of shape ($A \\times 1 \\times B \\times C \\times 1 \\times D$), the output tensor will be of shape($A \\times B \\times C \\times D$)."
      ],
      "metadata": {
        "id": "UdapRjr9mrht"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.ones((10, 1, 10))\n",
        "Y = X.squeeze()\n",
        "\n",
        "X.shape, Y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KhRfdIdWnpYe",
        "outputId": "361996ba-06bb-4ad1-fbdb-f1f4c8b2179f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([10, 1, 10]), torch.Size([10, 10]))"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* `torch.argmax()`: Returns the indices of the maximum value of all elements in the input tensor."
      ],
      "metadata": {
        "id": "epeAxlbko-VM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.tensor([[1, 2, 3],\n",
        "                  [4, 5, 6],\n",
        "                  [7, 8, 9]])\n",
        "X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JlqZ8_2apLah",
        "outputId": "c4cc0853-fe69-4e95-b681-fc5a088b9de7"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 2, 3],\n",
              "        [4, 5, 6],\n",
              "        [7, 8, 9]])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y = X.argmax()  # will retrun the argmax of flattened X\n",
        "Y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H4XD42cSpQul",
        "outputId": "0f2b3779-0f52-4029-a1ec-04fff33ac01a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(8)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y = X.argmax(dim=0)  # returns maximum indices across rows\n",
        "Y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g1AR4EtBpolc",
        "outputId": "4d0988b4-77b7-4327-a0f2-84af402b4e48"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([2, 2, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y = X.argmax(dim=1) # returns maximum indices across columns\n",
        "Y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUjDD-YLrVrs",
        "outputId": "14d530a1-df3e-4018-c243-6f2139ae4c5d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([2, 2, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* `math` library in python: this module provides access to the mathematical functions defined by the C standard.\n",
        "* `math.exp(x)`: return **e** raised to the power of x.\n",
        "* `math.pow(x, y)`: return x raised to the power of y."
      ],
      "metadata": {
        "id": "hTiQxkqlr927"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "math.exp(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1w5gtqG_s22m",
        "outputId": "5ac59332-b19f-4c9e-e6bf-cb5d13338ba8"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.718281828459045"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "math.pow(2, 3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-t3y_dXs---",
        "outputId": "24c816c0-a170-4352-9192-e09268e7b2c6"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8.0"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* `defaultdict` from `collections` module: `defaultdict` is a container like dictionaries. In fact, it is a subclass of the dictionary class that returns a dictionary like object.\n",
        "* Usually, a Python dictionary throws a `KeyError` if you try to get an item with a key that is not currently in the dictionary. In contrast, the `defaultdict` will simply create any item that you try to access.\n",
        "* To create such a default item, it calls the function object that you pass to the constructor."
      ],
      "metadata": {
        "id": "4yhz81Q5ydyc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "d = defaultdict(int)\n",
        "s = 'mississippi'\n",
        "\n",
        "for k in s:\n",
        "  d[k] += 1\n",
        "\n",
        "d.items(), d['i'], d['j']"
      ],
      "metadata": {
        "id": "CpJBCXd501OM",
        "outputId": "287f31ca-b087-43a1-97e2-1f258ab65b6a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(dict_items([('m', 1), ('i', 4), ('s', 4), ('p', 2), ('j', 0)]), 4, 0)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    }
  ]
}