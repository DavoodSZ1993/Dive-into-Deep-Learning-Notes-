{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNXYWlgXQZo6MZyXTXOB0V+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DavoodSZ1993/Dive-into-Deep-Learning-Notes-/blob/main/10_7_Encoder_Decoder_Machine_Translation_notes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JGly9mBzOLvA",
        "outputId": "9283aa02-d65e-4082-faec-c1d9a8b33270"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.0/93.0 KB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.0/121.0 KB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.6/83.6 KB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m40.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install d2l==1.0.0-alpha1.post0 --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10.7 Encoder-Decoder Seq2Seq for Machine Translation"
      ],
      "metadata": {
        "id": "ELgg6IAMOSJR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10.7.2 Encoder"
      ],
      "metadata": {
        "id": "q2PE_PYhVnZs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "input_size, hidden_size, num_layers = 10, 20, 2\n",
        "rnn = nn.GRU(input_size, hidden_size, num_layers)\n",
        "\n",
        "rnn._flat_weights_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eb4UJzfAVsqm",
        "outputId": "600daffd-01da-4f36-87c1-0e41efff4a7c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['weight_ih_l0',\n",
              " 'weight_hh_l0',\n",
              " 'bias_ih_l0',\n",
              " 'bias_hh_l0',\n",
              " 'weight_ih_l1',\n",
              " 'weight_hh_l1',\n",
              " 'bias_ih_l1',\n",
              " 'bias_hh_l1']"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rnn._parameters"
      ],
      "metadata": {
        "id": "aOOq8k38WtQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Embedding\n",
        "* Embedding layer is equivalent to a linear layer without the bias term. However, embedding does a lookup instead of a matrix-vector multiplication.\n",
        "\n",
        "* An embedding is an efficient alternative way to a single linear layer when one has a large number of input features. This may happen in natural language processing (NLP) when one is working with text data.\n",
        "\n",
        "* Class `torch.nn.Embedding(num_embeddings, embedding_dim)`: A simple lookup table that stores embeddings of a fixed dictionary and size.\n",
        "\n",
        "* This module is often used to score word embeddings and retrieve them using indices. The input to the module is a list of indices, and the output is the corresponding word embeddings.\n",
        "\n",
        "**Parameters:**\n",
        "\n",
        "* *num_embeddings(int)*: Size of the dictionary of embeddings.\n",
        "* *embedding_dim(int)*: The size of each embedding vector.\n",
        "\n",
        "**Shape:**\n",
        "\n",
        "* `input: (*)`: IntTensor or LongTensor of arbitrary shape containing the indices to extract.\n",
        "* `output(*, H)`: Where * is the input shape and H is `embedding_dim`. "
      ],
      "metadata": {
        "id": "8xyNSVT0XxIS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size, embed_size = 10, 8\n",
        "\n",
        "embedding = nn.Embedding(vocab_size, embed_size)"
      ],
      "metadata": {
        "id": "G6gKHt74e3kC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size, num_steps = 4, 9\n",
        "X = torch.zeros((batch_size, num_steps))\n",
        "\n",
        "embs = embedding(X.t().type(torch.int32))\n",
        "embs[0,:,:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOZGrMaMfrrE",
        "outputId": "bc350789-bdd8-4118-9ddd-ae1c052a28e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.1544,  0.7103,  0.2287,  1.2542,  0.0098,  0.2911, -0.1725, -1.0152],\n",
              "        [-0.1544,  0.7103,  0.2287,  1.2542,  0.0098,  0.2911, -0.1725, -1.0152],\n",
              "        [-0.1544,  0.7103,  0.2287,  1.2542,  0.0098,  0.2911, -0.1725, -1.0152],\n",
              "        [-0.1544,  0.7103,  0.2287,  1.2542,  0.0098,  0.2911, -0.1725, -1.0152]],\n",
              "       grad_fn=<SliceBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### In-depth Look at `nn.Embedding()`\n",
        "* The `nn.Embedding()` layer is a simple lookup table that maps an index value to a wieght matrix of a certain dimension.\n",
        "\n",
        "* This simple operation is the foundation of many advanced NLP architectures, allowing for the processing of discrete input symbols in a continuous state.\n",
        "\n",
        "* The *nn.Embedding* layer takes at least two arguments, **the vocabulary size** and **the size of the encoded representation for each word**.\n",
        "\n",
        "* Lets say we have a vocabulary of 1000 words, then the first argument would be 1000. Each word in the vocabulary will be represented by a vector of fixed size. So, the second argument is the size of the learned embedding for each word."
      ],
      "metadata": {
        "id": "Fkbf5BttVlJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size, embed_size = 10, 50\n",
        "\n",
        "embedding = nn.Embedding(vocab_size, embed_size)"
      ],
      "metadata": {
        "id": "Nl_m43L-MnnF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above example, PyTorch is created a lookup table named `embedding` that has 10 rows and 50 columns. Each row represents a single word embedding that is initialized randomly drawn from a uniform distribution. They are initialized using `nn.init.uniform_()` function from the `torch.nn.init` module and weights are initialized with random values between -1 and 1."
      ],
      "metadata": {
        "id": "z5UBX2pWNNr7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding(torch.LongTensor([0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50hYINUUOqAO",
        "outputId": "844af982-0981-49ea-ea07-dc416b695122"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.8511,  0.1879,  0.7696,  0.6570, -1.3819,  1.5425,  1.2204,  1.3877,\n",
              "         -0.1510, -1.0491,  0.0389, -0.0055, -0.2214,  1.0159,  0.8809,  0.5047,\n",
              "         -1.0305, -0.2009,  1.4088, -1.9277, -1.5470, -0.8555,  0.1258,  0.8923,\n",
              "          1.4227,  0.9337, -0.6947, -0.0893,  0.0046,  1.4121,  0.6984, -1.4294,\n",
              "          0.0077,  0.1171,  1.0447, -1.2901,  0.9510,  1.2457,  1.4315,  0.9397,\n",
              "          1.4418, -0.4111,  1.3055, -0.7426, -0.7994, -0.3499,  0.5569, -0.8989,\n",
              "         -0.5761,  1.3471]], grad_fn=<EmbeddingBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above numbers get tuned and optimized during the training process to convey the meaning of a certain word."
      ],
      "metadata": {
        "id": "f_3-svn5SH-5"
      }
    }
  ]
}