{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMD/sbmWKgr+SInqpjGdNGr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DavoodSZ1993/Dive-into-Deep-Learning-Notes-/blob/main/10_7_Encoder_Decoder_Machine_Translation_notes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JGly9mBzOLvA",
        "outputId": "9283aa02-d65e-4082-faec-c1d9a8b33270"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.0/93.0 KB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.0/121.0 KB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.6/83.6 KB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m40.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install d2l==1.0.0-alpha1.post0 --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10.7 Encoder-Decoder Seq2Seq for Machine Translation"
      ],
      "metadata": {
        "id": "ELgg6IAMOSJR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10.7.2 Encoder"
      ],
      "metadata": {
        "id": "q2PE_PYhVnZs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "input_size, hidden_size, num_layers = 10, 20, 2\n",
        "rnn = nn.GRU(input_size, hidden_size, num_layers)\n",
        "\n",
        "rnn._flat_weights_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eb4UJzfAVsqm",
        "outputId": "600daffd-01da-4f36-87c1-0e41efff4a7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['weight_ih_l0',\n",
              " 'weight_hh_l0',\n",
              " 'bias_ih_l0',\n",
              " 'bias_hh_l0',\n",
              " 'weight_ih_l1',\n",
              " 'weight_hh_l1',\n",
              " 'bias_ih_l1',\n",
              " 'bias_hh_l1']"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rnn._parameters"
      ],
      "metadata": {
        "id": "aOOq8k38WtQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Embedding\n",
        "* Embedding layer is equivalent to a linear layer without the bias term. However, embedding does a lookup instead of a matrix-vector multiplication.\n",
        "\n",
        "* An embedding is an efficient alternative way to a single linear layer when one has a large number of input features. This may happen in natural language processing (NLP) when one is working with text data.\n",
        "\n",
        "* Class `torch.nn.Embedding(num_embeddings, embedding_dim)`: A simple lookup table that stores embeddings of a fixed dictionary and size.\n",
        "\n",
        "* This module is often used to score word embeddings and retrieve them using indices. The input to the module is a list of indices, and the output is the corresponding word embeddings.\n",
        "\n",
        "**Parameters:**\n",
        "\n",
        "* *num_embeddings(int)*: Size of the dictionary of embeddings.\n",
        "* *embedding_dim(int)*: The size of each embedding vector.\n",
        "\n",
        "**Shape:**\n",
        "\n",
        "* `input: (*)`: IntTensor or LongTensor of arbitrary shape containing the indices to extract.\n",
        "* `output(*, H)`: Where * is the input shape and H is `embedding_dim`. "
      ],
      "metadata": {
        "id": "8xyNSVT0XxIS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size, embed_size = 10, 8\n",
        "\n",
        "embedding = nn.Embedding(vocab_size, embed_size)"
      ],
      "metadata": {
        "id": "G6gKHt74e3kC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size, num_steps = 4, 9\n",
        "X = torch.zeros((batch_size, num_steps))\n",
        "\n",
        "embs = embedding(X.t().type(torch.int32))\n",
        "embs[0,:,:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOZGrMaMfrrE",
        "outputId": "bc350789-bdd8-4118-9ddd-ae1c052a28e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.1544,  0.7103,  0.2287,  1.2542,  0.0098,  0.2911, -0.1725, -1.0152],\n",
              "        [-0.1544,  0.7103,  0.2287,  1.2542,  0.0098,  0.2911, -0.1725, -1.0152],\n",
              "        [-0.1544,  0.7103,  0.2287,  1.2542,  0.0098,  0.2911, -0.1725, -1.0152],\n",
              "        [-0.1544,  0.7103,  0.2287,  1.2542,  0.0098,  0.2911, -0.1725, -1.0152]],\n",
              "       grad_fn=<SliceBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### In-depth Look at `nn.Embedding()`\n",
        "* The `nn.Embedding()` layer is a simple lookup table that maps an index value to a wieght matrix of a certain dimension.\n",
        "\n",
        "* This simple operation is the foundation of many advanced NLP architectures, allowing for the processing of discrete input symbols in a continuous state.\n",
        "\n",
        "* The *nn.Embedding* layer takes at least two arguments, **the vocabulary size** and **the size of the encoded representation for each word**.\n",
        "\n",
        "* Lets say we have a vocabulary of 1000 words, then the first argument would be 1000. Each word in the vocabulary will be represented by a vector of fixed size. So, the second argument is the size of the learned embedding for each word."
      ],
      "metadata": {
        "id": "Fkbf5BttVlJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size, embed_size = 10, 50\n",
        "\n",
        "embedding = nn.Embedding(vocab_size, embed_size)"
      ],
      "metadata": {
        "id": "Nl_m43L-MnnF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above example, PyTorch is created a lookup table named `embedding` that has 10 rows and 50 columns. Each row represents a single word embedding that is initialized randomly drawn from a uniform distribution. They are initialized using `nn.init.uniform_()` function from the `torch.nn.init` module and weights are initialized with random values between -1 and 1."
      ],
      "metadata": {
        "id": "z5UBX2pWNNr7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding(torch.LongTensor([0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50hYINUUOqAO",
        "outputId": "844af982-0981-49ea-ea07-dc416b695122"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.8511,  0.1879,  0.7696,  0.6570, -1.3819,  1.5425,  1.2204,  1.3877,\n",
              "         -0.1510, -1.0491,  0.0389, -0.0055, -0.2214,  1.0159,  0.8809,  0.5047,\n",
              "         -1.0305, -0.2009,  1.4088, -1.9277, -1.5470, -0.8555,  0.1258,  0.8923,\n",
              "          1.4227,  0.9337, -0.6947, -0.0893,  0.0046,  1.4121,  0.6984, -1.4294,\n",
              "          0.0077,  0.1171,  1.0447, -1.2901,  0.9510,  1.2457,  1.4315,  0.9397,\n",
              "          1.4418, -0.4111,  1.3055, -0.7426, -0.7994, -0.3499,  0.5569, -0.8989,\n",
              "         -0.5761,  1.3471]], grad_fn=<EmbeddingBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above numbers get tuned and optimized during the training process to convey the meaning of a certain word."
      ],
      "metadata": {
        "id": "f_3-svn5SH-5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* `torch.tensor.repeat()`: Repeats a tensor along the specified dimensions."
      ],
      "metadata": {
        "id": "xk3w-1k7Ujtx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "X = torch.tensor([1, 2])\n",
        "\n",
        "X.repeat(2, 2), X.repeat(2, 2).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xy_ccMQiVIqA",
        "outputId": "4e9944b6-a43e-4024-d1d3-9dff25af7c01"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[1, 2, 1, 2],\n",
              "         [1, 2, 1, 2]]), torch.Size([2, 4]))"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `torch.optim` package\n",
        "This is a package implementing various optimization algorithms."
      ],
      "metadata": {
        "id": "8J1Gk0nuX_R-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adam Optimizer\n",
        "* The Adam optimizer algorithm is an extension to stochastic gradient descent.\n",
        "* Stochastic gradient descent maintains a single learning rate for all weight updates and does not change during training.\n",
        "* In Adam, a learning rate is maintained for each network weight (parameter) and seperately adapted as learning unfolds.\n",
        "* Adam exploits the advantages of **Adaptive Gradient Algorithm** and **Root Mean Square Propagation** which are two extensions of stochastic gradient descent.\n",
        "* **Adaptive Gradient Algorithm**: maintains a per-parameter learning rate that improves performance on problems with sparse gradients (e.g. natural language processing, and computer vision problems.)\n",
        "* **Root Mean Square Propagation**: also maintains a per=parameter learning rate that are adapted based on the average of recent magnitudes of the gradients for the weight (e.g. how quickly it is changing).\n",
        "* In Adam, instead of adapting the parameter learning rates based on the average first moment (the mean) as in **Root Mean Square Propogation**, the algorithm makes use of the average of the second moments of the gradients (the uncentered variance).\n",
        "* Adam algorithm calculates an exponential moving average of the gradient and the squared gradient, and the parameters `beta1` and `beta2` control the decay rates of the moving averages."
      ],
      "metadata": {
        "id": "IOUqt-L0cfwD"
      }
    }
  ]
}