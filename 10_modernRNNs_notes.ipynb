{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOIZ2l1RtXhtw82CU7d8ebf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DavoodSZ1993/Dive-into-Deep-Learning-Notes-/blob/main/10_modernRNNs_notes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H-cVYNOwViWt"
      },
      "outputs": [],
      "source": [
        "!pip install d2l==1.0.0-alpha1.post0 --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* RNNs are faced with two numerical stability problems: exploding and vanishing gradients.\n",
        "* For exploding gradients, the gradient clipping method was introduced in the previous chapter"
      ],
      "metadata": {
        "id": "79s36Y9VV6h8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10.1 Long Short-Term Memory (LSTM)\n",
        "\n",
        "* Long short-term memory (LSTM) was used to address the vanishing gradients in RNNs.\n",
        "* Recurrent Neural Networks suffer from short-term memory due to vanishing gradient problem, especially when working with large data sequences.\n",
        "* Newer versions of RNNs can preserve important information from earlier parts of the sequence and carry it forward."
      ],
      "metadata": {
        "id": "bjZQyqDwX738"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LSTMs\n",
        "\n",
        "Standard RNNs and LSTMs use recurrent units to learn from sequential data. But, the structure of the recurrent unit is very different. Recurrent structure in the standard RNNs has the following structure:\n",
        "$$\n",
        "ùêá_t = Œ¶[ùêó_tùêñ_{xh} + ùêá_{t-1}ùêñ_{hh} + ùêõ_h]\n",
        "$$\n",
        "LSTM employs various gates to decide what information to keep or discard. Also, it adds a **cell state**, which is like a long term memory of LSTM."
      ],
      "metadata": {
        "id": "nvS9bgfYz0Uf"
      }
    }
  ]
}