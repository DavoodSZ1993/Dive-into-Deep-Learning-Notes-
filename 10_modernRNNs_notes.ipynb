{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM+2B81TzjhoxE362uPQNlA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DavoodSZ1993/Dive-into-Deep-Learning-Notes-/blob/main/10_modernRNNs_notes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H-cVYNOwViWt"
      },
      "outputs": [],
      "source": [
        "!pip install d2l==1.0.0-alpha1.post0 --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* RNNs are faced with two numerical stability problems: exploding and vanishing gradients.\n",
        "* For exploding gradients, the gradient clipping method was introduced in the previous chapter"
      ],
      "metadata": {
        "id": "79s36Y9VV6h8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10.1 Long Short-Term Memory (LSTM)\n",
        "\n",
        "* Long short-term memory (LSTM) was used to address the vanishing gradients in RNNs.\n",
        "* Recurrent Neural Networks suffer from short-term memory due to vanishing gradient problem, especially when working with large data sequences.\n",
        "* Newer versions of RNNs can preserve important information from earlier parts of the sequence and carry it forward."
      ],
      "metadata": {
        "id": "bjZQyqDwX738"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LSTMs\n",
        "\n",
        "Standard RNNs and LSTMs use recurrent units to learn from sequential data. But, the structure of the recurrent unit is very different. Recurrent structure in the standard RNNs has the following structure:\n",
        "$$\n",
        "ğ‡_t = Î¦[ğ—_tğ–_{xh} + ğ‡_{t-1}ğ–_{hh} + ğ›_h]\n",
        "$$\n",
        "LSTM employs various gates to decide what information to keep or discard. Also, it adds a **cell state**, which is like a long term memory of LSTM. The recurrent unit of LSTM is more complex than RNNs and contains the following parts:\n",
        "\n",
        "* **Hidden state** ($ğ‡_{t-1}$) & new inputs ($ğ—_t$): These two are concatenated before going through different gates.\n",
        "\n",
        "* **Forget gate**: Controls what information should be forgotten.\n",
        "$$\n",
        "ğ…_t = Ïƒ[ğ—_tW_{xf} + ğ‡_{t-1}ğ–_{hf} + ğ›_f]\n",
        "$$\n",
        "Since sigmoid function ranges between 0 and 1, it sets which value in **the cell state** should be forgotten (multiplied by 0), remembered (multiplied by 1), or partially remembered (multiplied by some value between zero and one).\n",
        "\n",
        "* **input gate**: Helps to identify important elements that need to be added to cell state.\n",
        "$$\n",
        "ğˆ_t = Ïƒ[ğ—_tW_{xi} + ğ‡_{t-1}ğ–_{hi} + ğ›_i]\n",
        "$$\n",
        "Intuitively, the input gate determines how much of the input node's value should be added to the current memory cell internal state.\n",
        "\n",
        "* **Input node**:\n",
        "$$\n",
        "ğ‚Ìƒ_t = tanh[ğ—_tW_{xc} + ğ‡_{t-1}ğ–_{hc} + ğ›_c]\n",
        "$$\n",
        "\n",
        "* **Cell state** ($ğ‚_t$): First, the previous cell state ($ğ‚_{t-1}$) gets multiplied by the result of the forget gate (determines how much of the old cell internal state we retain.) Then we add new information from [input gate * cell state candidate] to get the cell sate (This governs how much we take new data into account.)\n",
        "$$\n",
        "ğ‚_t = ğ…_t âŠ— ğ‚_{t-1} + ğˆ_t âŠ— ğ‚Ìƒ_t\n",
        "$$\n",
        "\n",
        "So, in order to update the hidden state ($ğ‡_t$), the follwing formula can be utilized:\n",
        "$$\n",
        "ğ‡_t = ğ_t âŠ— tanh(ğ‚_t)\n",
        "$$\n",
        "\n"
      ],
      "metadata": {
        "id": "nvS9bgfYz0Uf"
      }
    }
  ]
}