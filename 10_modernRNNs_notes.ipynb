{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNI1KQS13j6FV9wwaIx6PUw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DavoodSZ1993/Dive-into-Deep-Learning-Notes-/blob/main/10_modernRNNs_notes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "H-cVYNOwViWt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84eb3ba4-20cb-4aa0-cc6d-65c347a00147"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.0/93.0 KB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.0/121.0 KB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.6/83.6 KB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install d2l==1.0.0-alpha1.post0 --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* RNNs are faced with two numerical stability problems: exploding and vanishing gradients.\n",
        "* For exploding gradients, the gradient clipping method was introduced in the previous chapter"
      ],
      "metadata": {
        "id": "79s36Y9VV6h8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10.1 Long Short-Term Memory (LSTM)\n",
        "\n",
        "* Long short-term memory (LSTM) was used to address the vanishing gradients in RNNs.\n",
        "* Recurrent Neural Networks suffer from short-term memory due to vanishing gradient problem, especially when working with large data sequences.\n",
        "* Newer versions of RNNs can preserve important information from earlier parts of the sequence and carry it forward."
      ],
      "metadata": {
        "id": "bjZQyqDwX738"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LSTMs\n",
        "\n",
        "Standard RNNs and LSTMs use recurrent units to learn from sequential data. But, the structure of the recurrent unit is very different. Recurrent structure in the standard RNNs has the following structure:\n",
        "$$\n",
        "𝐇_t = Φ[𝐗_t𝐖_{xh} + 𝐇_{t-1}𝐖_{hh} + 𝐛_h]\n",
        "$$\n",
        "LSTM employs various gates to decide what information to keep or discard. Also, it adds a **cell state**, which is like a long term memory of LSTM. The recurrent unit of LSTM is more complex than RNNs and contains the following parts:\n",
        "\n",
        "* **Hidden state** ($𝐇_{t-1}$) & new inputs ($𝐗_t$): These two are concatenated before going through different gates.\n",
        "\n",
        "* **Forget gate**: Controls what information should be forgotten.\n",
        "$$\n",
        "𝐅_t = σ[𝐗_tW_{xf} + 𝐇_{t-1}𝐖_{hf} + 𝐛_f]\n",
        "$$\n",
        "Since sigmoid function ranges between 0 and 1, it sets which value in **the cell state** should be forgotten (multiplied by 0), remembered (multiplied by 1), or partially remembered (multiplied by some value between zero and one).\n",
        "\n",
        "* **input gate**: Helps to identify important elements that need to be added to cell state.\n",
        "$$\n",
        "𝐈_t = σ[𝐗_tW_{xi} + 𝐇_{t-1}𝐖_{hi} + 𝐛_i]\n",
        "$$\n",
        "Intuitively, the input gate determines how much of the input node's value should be added to the current memory cell internal state.\n",
        "\n",
        "* **Input node**:\n",
        "$$\n",
        "𝐂̃_t = tanh[𝐗_tW_{xc} + 𝐇_{t-1}𝐖_{hc} + 𝐛_c]\n",
        "$$\n",
        "\n",
        "* **Cell state** ($𝐂_t$): First, the previous cell state ($𝐂_{t-1}$) gets multiplied by the result of the forget gate (determines how much of the old cell internal state we retain.) Then we add new information from [input gate * cell state candidate] to get the cell sate (This governs how much we take new data into account.)\n",
        "$$\n",
        "𝐂_t = 𝐅_t ⊗ 𝐂_{t-1} + 𝐈_t ⊗ 𝐂̃_t\n",
        "$$\n",
        "\n",
        "So, in order to update the hidden state ($𝐇_t$), the follwing formula can be utilized:\n",
        "$$\n",
        "𝐇_t = 𝐎_t ⊗ tanh(𝐂_t)\n",
        "$$\n",
        "\n"
      ],
      "metadata": {
        "id": "nvS9bgfYz0Uf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Class `torch.nn.LSTM(*args(input_size, hidden_size), *kwargs)`: Applies a multi-layer long short-term memory (LSTM) RNN to an input sequence. \n",
        "* `input_size`: The number of expected features in the input `x`.\n",
        "* `hidden_size`: The number of features in the hidden state `h`."
      ],
      "metadata": {
        "id": "byDMDLgDYyAA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inputs: input, (hidden state (`h_0`), cell state (`c_0`))\n",
        "* `input`: tensor of shape ($L, H_{in}$) for unbatched input, ($L,N,H_{in}$), when `batch_first=False` or ($N, L, H_{in}$) when `batch_first=True` containing the features of the input sequence. Where $N$ is batch size, $L$ is sequence length, and $H_{in}$ is input size.\n",
        "* `h_o`: tensor of shape ($D$ * *num_layers*, $H_{out}$) for unbatched input, ($D$* *num_layers*, $N$, $H_{out}$) containing the initial hidden state for each element in the input sequence. Default to zero if (`h_o`, `c_o`) is not provided. Where $D$ is equal to 2 if bidirectional=True, otherwise equals to 1, and $H_{out}$ is hidden size.\n",
        "* `c_o`: tensor of shape ($D$ * *num_layers*, $H_{cell}$) for unbatched input, or ($D$ * *num_layers*, $N$, $H_{cell}$) containing the initial cell state for each element in the input sequence. Where $H_{cell}$ is hidden size."
      ],
      "metadata": {
        "id": "5nLI--HRZsd4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Outputs: output, (`h_n`, `c_n`)\n",
        "* `output`: Tensor of shape ($L$, $D$ * $H_{out}$) for unbatched input, ($L$, $N$, $D$ * $H_{out}$) when `batch_first=False`, or ($N$, $L$, $D$ * $H_{out}$) when `batch_first=True` contains the output features (`h_t`) from the last layer of the LSTM, for each t.\n",
        "* `h_n`: Tensor of shape ($D$ * *num_layers*, $H_{out}$) for unbatched input, or ($D$ * *num_layers*, $N$, $H_{out}$) containing the final hidden state for each element in the sequence.\n",
        "* `c_n`: Tensor of shape ($D$ * *num_layers*, $H_{cell}$) for unbatched input or ($D$ * *num_layers*, $N$, $H_{cell}$) containing the final cell state for each element in the sequence."
      ],
      "metadata": {
        "id": "EWop-qA7fCpT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "rnn = nn.LSTM(10, 20)                # (input_size, hidden_size)\n",
        "input = torch.randn(5, 3, 10)        # (sequence length, batch_size, input_size)\n",
        "h0 = torch.randn(1, 3, 20)           # (D * num_layers, batch_size, hidden_size)\n",
        "c0 = torch.rand(1, 3, 20)            # (D * num_layers, batch_size, hidden_size)\n",
        "\n",
        "output, (hn, cn) = rnn(input, (h0, c0))\n",
        "\n",
        "print('output shape: ', output.shape) # (sequence length, batch_size, D * hidden_size) == (5, 3, 20)\n",
        "print('h_n shape:', hn.shape)         # (D * num_layers, batch_size, hidden_size) == (1, 3, 20)\n",
        "print('c_n shape:', cn.shape)         # (D * num_layers, batch_size, hidden_size) == (1, 3, 20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pzC-_WTMi0QP",
        "outputId": "6f6d31e6-410a-4495-d8e9-f5ca5fede423"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "output shape:  torch.Size([5, 3, 20])\n",
            "h_n shape: torch.Size([1, 3, 20])\n",
            "c_n shape: torch.Size([1, 3, 20])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10.2 Gated Recurrent Units"
      ],
      "metadata": {
        "id": "VDbO6SBj5fo5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Class `torch.nn.GRU(*args (input_size, hidden_size), **kwargs)`: Applies a multi-layer gated recrurrent unit (GRU) RNN to an input sequence."
      ],
      "metadata": {
        "id": "PB5e-q2T5lXJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inputs: `input`, `h_0`\n",
        "* `input`: tensor of shape ($L, H_{in}$) for unbatched input, ($L, N, H_{in}$) when `batch_first=False` or ($N,L,H_{in}$) when `batch_first=True` containing the features of the input sequence. Where $L$ is sequence length, $N$ is batch size, and $H_{in}$ is input size.\n",
        "* `h_0`: Tensor of shape ($D$ * *num_layers*, $H_{out}$) or ($D$ * *num_layers*,$N$, $H_{out}$) containing the initial hidden state for the input sequence. Wehre $D$ is equal to 2 if bidirectional=True, otherwise is equal to 1, and $H_{out}$ is hidden size."
      ],
      "metadata": {
        "id": "B7l2yCpH7dcx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Outputs: `output`, `h_n`\n",
        "* `output`: tensor of shape ($L$, $D$ * $H_{out}$) for unbatched input, ($L$, $N$, $D$ * $H_{out}$) when `batch_first=False` or ($N$, $L$, $D$ * $H_{out}$) when `batch_first=True` containing the output features (`h_t`) from the last layer of the GRU, for each t.\n",
        "* `h_n`: tensor of shape ($D$ * *num_layers*, $H_{out}$) or ($D$ * *num_layers*, $N$, $H_{out}$) containing the final hidden state for the input sequence."
      ],
      "metadata": {
        "id": "gyvBYQfJ9b9x"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KQSt1-Jo_lBP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}