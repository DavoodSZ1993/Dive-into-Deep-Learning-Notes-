{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNI1KQS13j6FV9wwaIx6PUw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DavoodSZ1993/Dive-into-Deep-Learning-Notes-/blob/main/10_modernRNNs_notes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "H-cVYNOwViWt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84eb3ba4-20cb-4aa0-cc6d-65c347a00147"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m93.0/93.0 KB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m121.0/121.0 KB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m83.6/83.6 KB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install d2l==1.0.0-alpha1.post0 --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* RNNs are faced with two numerical stability problems: exploding and vanishing gradients.\n",
        "* For exploding gradients, the gradient clipping method was introduced in the previous chapter"
      ],
      "metadata": {
        "id": "79s36Y9VV6h8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10.1 Long Short-Term Memory (LSTM)\n",
        "\n",
        "* Long short-term memory (LSTM) was used to address the vanishing gradients in RNNs.\n",
        "* Recurrent Neural Networks suffer from short-term memory due to vanishing gradient problem, especially when working with large data sequences.\n",
        "* Newer versions of RNNs can preserve important information from earlier parts of the sequence and carry it forward."
      ],
      "metadata": {
        "id": "bjZQyqDwX738"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LSTMs\n",
        "\n",
        "Standard RNNs and LSTMs use recurrent units to learn from sequential data. But, the structure of the recurrent unit is very different. Recurrent structure in the standard RNNs has the following structure:\n",
        "$$\n",
        "ğ‡_t = Î¦[ğ—_tğ–_{xh} + ğ‡_{t-1}ğ–_{hh} + ğ›_h]\n",
        "$$\n",
        "LSTM employs various gates to decide what information to keep or discard. Also, it adds a **cell state**, which is like a long term memory of LSTM. The recurrent unit of LSTM is more complex than RNNs and contains the following parts:\n",
        "\n",
        "* **Hidden state** ($ğ‡_{t-1}$) & new inputs ($ğ—_t$): These two are concatenated before going through different gates.\n",
        "\n",
        "* **Forget gate**: Controls what information should be forgotten.\n",
        "$$\n",
        "ğ…_t = Ïƒ[ğ—_tW_{xf} + ğ‡_{t-1}ğ–_{hf} + ğ›_f]\n",
        "$$\n",
        "Since sigmoid function ranges between 0 and 1, it sets which value in **the cell state** should be forgotten (multiplied by 0), remembered (multiplied by 1), or partially remembered (multiplied by some value between zero and one).\n",
        "\n",
        "* **input gate**: Helps to identify important elements that need to be added to cell state.\n",
        "$$\n",
        "ğˆ_t = Ïƒ[ğ—_tW_{xi} + ğ‡_{t-1}ğ–_{hi} + ğ›_i]\n",
        "$$\n",
        "Intuitively, the input gate determines how much of the input node's value should be added to the current memory cell internal state.\n",
        "\n",
        "* **Input node**:\n",
        "$$\n",
        "ğ‚Ìƒ_t = tanh[ğ—_tW_{xc} + ğ‡_{t-1}ğ–_{hc} + ğ›_c]\n",
        "$$\n",
        "\n",
        "* **Cell state** ($ğ‚_t$): First, the previous cell state ($ğ‚_{t-1}$) gets multiplied by the result of the forget gate (determines how much of the old cell internal state we retain.) Then we add new information from [input gate * cell state candidate] to get the cell sate (This governs how much we take new data into account.)\n",
        "$$\n",
        "ğ‚_t = ğ…_t âŠ— ğ‚_{t-1} + ğˆ_t âŠ— ğ‚Ìƒ_t\n",
        "$$\n",
        "\n",
        "So, in order to update the hidden state ($ğ‡_t$), the follwing formula can be utilized:\n",
        "$$\n",
        "ğ‡_t = ğ_t âŠ— tanh(ğ‚_t)\n",
        "$$\n",
        "\n"
      ],
      "metadata": {
        "id": "nvS9bgfYz0Uf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Class `torch.nn.LSTM(*args(input_size, hidden_size), *kwargs)`: Applies a multi-layer long short-term memory (LSTM) RNN to an input sequence. \n",
        "* `input_size`: The number of expected features in the input `x`.\n",
        "* `hidden_size`: The number of features in the hidden state `h`."
      ],
      "metadata": {
        "id": "byDMDLgDYyAA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inputs: input, (hidden state (`h_0`), cell state (`c_0`))\n",
        "* `input`: tensor of shape ($L, H_{in}$) for unbatched input, ($L,N,H_{in}$), when `batch_first=False` or ($N, L, H_{in}$) when `batch_first=True` containing the features of the input sequence. Where $N$ is batch size, $L$ is sequence length, and $H_{in}$ is input size.\n",
        "* `h_o`: tensor of shape ($D$ * *num_layers*, $H_{out}$) for unbatched input, ($D$* *num_layers*, $N$, $H_{out}$) containing the initial hidden state for each element in the input sequence. Default to zero if (`h_o`, `c_o`) is not provided. Where $D$ is equal to 2 if bidirectional=True, otherwise equals to 1, and $H_{out}$ is hidden size.\n",
        "* `c_o`: tensor of shape ($D$ * *num_layers*, $H_{cell}$) for unbatched input, or ($D$ * *num_layers*, $N$, $H_{cell}$) containing the initial cell state for each element in the input sequence. Where $H_{cell}$ is hidden size."
      ],
      "metadata": {
        "id": "5nLI--HRZsd4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Outputs: output, (`h_n`, `c_n`)\n",
        "* `output`: Tensor of shape ($L$, $D$ * $H_{out}$) for unbatched input, ($L$, $N$, $D$ * $H_{out}$) when `batch_first=False`, or ($N$, $L$, $D$ * $H_{out}$) when `batch_first=True` contains the output features (`h_t`) from the last layer of the LSTM, for each t.\n",
        "* `h_n`: Tensor of shape ($D$ * *num_layers*, $H_{out}$) for unbatched input, or ($D$ * *num_layers*, $N$, $H_{out}$) containing the final hidden state for each element in the sequence.\n",
        "* `c_n`: Tensor of shape ($D$ * *num_layers*, $H_{cell}$) for unbatched input or ($D$ * *num_layers*, $N$, $H_{cell}$) containing the final cell state for each element in the sequence."
      ],
      "metadata": {
        "id": "EWop-qA7fCpT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "rnn = nn.LSTM(10, 20)                # (input_size, hidden_size)\n",
        "input = torch.randn(5, 3, 10)        # (sequence length, batch_size, input_size)\n",
        "h0 = torch.randn(1, 3, 20)           # (D * num_layers, batch_size, hidden_size)\n",
        "c0 = torch.rand(1, 3, 20)            # (D * num_layers, batch_size, hidden_size)\n",
        "\n",
        "output, (hn, cn) = rnn(input, (h0, c0))\n",
        "\n",
        "print('output shape: ', output.shape) # (sequence length, batch_size, D * hidden_size) == (5, 3, 20)\n",
        "print('h_n shape:', hn.shape)         # (D * num_layers, batch_size, hidden_size) == (1, 3, 20)\n",
        "print('c_n shape:', cn.shape)         # (D * num_layers, batch_size, hidden_size) == (1, 3, 20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pzC-_WTMi0QP",
        "outputId": "6f6d31e6-410a-4495-d8e9-f5ca5fede423"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "output shape:  torch.Size([5, 3, 20])\n",
            "h_n shape: torch.Size([1, 3, 20])\n",
            "c_n shape: torch.Size([1, 3, 20])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10.2 Gated Recurrent Units"
      ],
      "metadata": {
        "id": "VDbO6SBj5fo5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Class `torch.nn.GRU(*args (input_size, hidden_size), **kwargs)`: Applies a multi-layer gated recrurrent unit (GRU) RNN to an input sequence."
      ],
      "metadata": {
        "id": "PB5e-q2T5lXJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inputs: `input`, `h_0`\n",
        "* `input`: tensor of shape ($L, H_{in}$) for unbatched input, ($L, N, H_{in}$) when `batch_first=False` or ($N,L,H_{in}$) when `batch_first=True` containing the features of the input sequence. Where $L$ is sequence length, $N$ is batch size, and $H_{in}$ is input size.\n",
        "* `h_0`: Tensor of shape ($D$ * *num_layers*, $H_{out}$) or ($D$ * *num_layers*,$N$, $H_{out}$) containing the initial hidden state for the input sequence. Wehre $D$ is equal to 2 if bidirectional=True, otherwise is equal to 1, and $H_{out}$ is hidden size."
      ],
      "metadata": {
        "id": "B7l2yCpH7dcx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Outputs: `output`, `h_n`\n",
        "* `output`: tensor of shape ($L$, $D$ * $H_{out}$) for unbatched input, ($L$, $N$, $D$ * $H_{out}$) when `batch_first=False` or ($N$, $L$, $D$ * $H_{out}$) when `batch_first=True` containing the output features (`h_t`) from the last layer of the GRU, for each t.\n",
        "* `h_n`: tensor of shape ($D$ * *num_layers*, $H_{out}$) or ($D$ * *num_layers*, $N$, $H_{out}$) containing the final hidden state for the input sequence."
      ],
      "metadata": {
        "id": "gyvBYQfJ9b9x"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KQSt1-Jo_lBP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}