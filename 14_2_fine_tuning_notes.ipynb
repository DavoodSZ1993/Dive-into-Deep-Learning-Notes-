{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNSszvRgjYVnLOambvMghdO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DavoodSZ1993/Dive-into-Deep-Learning-Notes-/blob/main/14_2_fine_tuning_notes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Prp1Xq00yIY",
        "outputId": "a4b26aa4-2e9d-44e5-e8fc-7559567a7847"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.0/93.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.9/121.9 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.9/84.9 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install d2l==1.0.0-alpha1.post0 --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 14.2 Fine-Tuning"
      ],
      "metadata": {
        "id": "YOszC6xI05Q2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Class `torchvision.datasets.ImageFolder(root, transforms)`: A generic data loader where the images are arranged in this way by default.\n",
        "\n",
        "``` \n",
        "root/dog/xxx.png\n",
        "root/dog/xxy.png\n",
        "root/dog/[...]/xxz.png\n",
        "\n",
        "root/cat/123.png\n",
        "root/cat/asdf3.png\n",
        "root/cat/[...]/asd932_.png\n",
        "```\n",
        "* root (string): Root directory path.\n",
        "* transform (callable, optional): A function/transform that takes in an PIL image and returns a transformed version.\n",
        "* The data should be in a different folder per label for the default PyTorch `ImageFolder` to load it correctly."
      ],
      "metadata": {
        "id": "rPinuhH06Twz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Class `torch.transforms.Normalize(mean, std)`: Normalize a tensor image with mean and standard deviation. This transform does not support PIL image.\n",
        "* **mean (sequence)**: Sequence of mean for each channel.\n",
        "* **std (sequence)**: Sequence of standard deviation for each channel."
      ],
      "metadata": {
        "id": "un78ZlAW-6aM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Class `torchvision.transforms.Resize(size)`: Resize the input image to the given size.\n",
        "* **size (sequence or int)**: Desired output size."
      ],
      "metadata": {
        "id": "BB3Xipj-AMbJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Class `torchvision.transforms.CenterCrop(size)`: Crops the given image at the center.\n",
        "* **size (sequence or int)**: Desired output size of the crop."
      ],
      "metadata": {
        "id": "-30RgBR2A82R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* `torchvision.models`: This package contains definitions of models for addressing different tasks including image classifications.\n",
        "* `torchvision.models.resnet18()`: ResNet-18 from Deep Residual Learning for Image Recognition."
      ],
      "metadata": {
        "id": "o1HuiAm6COQe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "\n",
        "model = torchvision.models.resnet18(pretrained=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VYeC1Whc08J_",
        "outputId": "560290e2-8964-4fd8-edbe-158af0db02f0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 82.2MB/s]\n"
          ]
        }
      ]
    }
  ]
}