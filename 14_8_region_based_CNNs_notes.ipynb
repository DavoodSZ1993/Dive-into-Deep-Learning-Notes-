{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPbwIjPDX236jpOcSyQeqUa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DavoodSZ1993/Dive-into-Deep-Learning-Notes-/blob/main/14_8_region_based_CNNs_notes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X3RCQ-yUQd89",
        "outputId": "1710bc40-26ed-414c-c4fd-d40694e38fcb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.0/93.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.9/121.9 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.9/84.9 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install d2l==1.0.0-alpha1.post0 --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 14.8 Region-based CNNs (R-CNNs)"
      ],
      "metadata": {
        "id": "lcQNJileQlBI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 14.8.2 Fast R-CNN"
      ],
      "metadata": {
        "id": "WyhUaJxCQpjD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "fQ0HydaRRlyc"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rois = torch.Tensor([[0, 0, 0, 20, 20],\n",
        "                     [0, 0, 10, 30, 30]])\n",
        "rois.dtype"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xrhe34bhRnit",
        "outputId": "dcfa014d-909d-4ec6-a7b5-f1e69202e64d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.float32"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rois = torch.tensor([[0, 0, 0, 20, 20],\n",
        "                     [0, 0, 10, 30, 30]])\n",
        "rois.dtype"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S03OUU3VRy0k",
        "outputId": "203a8f65-7092-4796-b020-9eb1ec2fc554"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.int64"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* `torchvision.ops`: Implements operators, losses, and layers that are specific for Computer Vision.\n",
        "* `torchvision.ops.roi_pool(input, boxes, output_size, spatial_factor)`: Performs Regions of Interest (RoI) pool operator described in Fast R-CNN.\n",
        "* input ([$N, C, H, W$]): the input tensor, i.e. a batch with $N$ elements. Each element contains $C$ feature maps of dimension $H \\times W$.\n",
        "* boxes (tensor[k, 5]): The box coordinates in ($x1, y1, x2, y2$) format where the regions will be taken from.\n",
        "* output_size (int or tuple[int, int]): The size of the output after the cropping is performed, as (height, width).\n",
        "* spatial_scale (float): a scaling factor that maps the box coordinates to the input coordinates. For example, if your boxes are defined on the scale of a $224 \\times 224$ image and your input is a $112 \\times 112$ feature map (resulting from a 0.5x scaling of the original image), you'll want to set this to 0.5. Default: 1.0.\n",
        "* Returns: The pooled RoIs (tensor[K, C, output_size[0], output_size[1)."
      ],
      "metadata": {
        "id": "K8MAWNUeSOjq"
      }
    }
  ]
}